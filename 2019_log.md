
20190530
------------------------------------------------------------------------------------------------------------------------
1. 三资单 投产
2. 小微贷作业整理，设计文档整理
3. 现金券单 准备大量moia配置
4. 银企对账单投产试跑
5. 交易对手抽数

ECC ： 
    处理三资的补数                   
	银企对账试跑
	现金券单的环境查询

20190531
------------------------------------------------------------------------------------------------------------------------
1. 三资单投产完毕  只上高要的，后期再派单修改
2. 小微贷等待会芳最后统一整理
3. 现金券单 ECC检查完毕 还需配置moia
4. 银企对账单投产完毕
5. 新接三个单 需求分析中

下周安排：
1. 主要是三个抽数单
2. 现金券投产

20190603
------------------------------------------------------------------------------------------------------------------------
1. 悦农微贷的接入文件单，整理文档
2. 交易对手的单
3. 现金券部署到生产
4. 确认存款+理财 还是只需要存款


私人：
1. 安装db2 数据库     (20200303 已安装)
2. 安装开发环境       (20200303 已安装)
3. 整理好文档，打通工作和生活之间的数据联通(20200303 数据文档分为2个 
yx当前任务.txt   工作日志 记录日常事项
个人总结.docx    记录技术以及思想
数据备份  hj[YYYYMMDD].rar
)

20190604
------------------------------------------------------------------------------------------------------------------------
1. 整理抽取年日均余额的sql
2. 张雅玲投产，借机检查自己的作业

20190605
------------------------------------------------------------------------------------------------------------------------
1. 由于裤子和鞋子都不合适，中午去寄了快递，退货了
2. 中午没有午睡，安装了nginx 重新编译安装的
3. 现金券的单确定加一个法人字段作为主键
4. 存款的数据太多，抽不起来，改用多次抽取方式
5. 交易对手的单 由于多人操作，导致我无法操作，耽搁了很久
6. AIA的同事谈论新工作的问题，又关注了一下
7. 天气问题，中午回去差点淋湿了

20190606
------------------------------------------------------------------------------------------------------------------------
1. 解决三个单的问题，目前已经时拟生效单

下周安排：无安排 即等待分配开发单

20190610
------------------------------------------------------------------------------------------------------------------------
1. 三资单 路径错了，加上了路径，导致这边提了意见
2. 现金券的测试一下增量，以及入库试跑
3. 整理中间表的临时表 为以后针对客户查询提供一个快速通道

20190611
------------------------------------------------------------------------------------------------------------------------


20190620
------------------------------------------------------------------------------------------------------------------------
1. CRM 中转数据文件的单评审
2. 零余额睡眠户和无主户的账号数据抽取


20190621
------------------------------------------------------------------------------------------------------------------------
1. 

每天都下载文件，然后一天下载23次，如何设计
#  filename down_data.sh
#  实现功能：将数据以json文件的方式下载下来
#  
分析：读取配置文件，获取上一次读取的值，然后设置这次下载的文件名

配置一个文件，记录一个值，然后循环读取，循环设值，1-23-1 的设置取值内容



确定文件文为

bankgold_YYYYMMDD_A.json
bankgold_YYYYMMDD_B.json




cd /home/hunter/data
par_date=`date +%Y%m%d`
file=/home/hunter/data/${par_date}
cfg_file=/home/hunter/data/file.cfg

cat ${cfg_file}| while read LINE
do
    recordline=`echo $LINE|sed -s/\"//g | awk -F , '{print $1}'`
done

20190624
------------------------------------------------------------------------------------------------------------------------
1. 周末因为眼镜被摔，啥也没干
2. 检查自助设备那块环境
3. 检查6个脚本运行情况
4. 检查风控脚本

5. wind 数据出现问题，业务提了保障一是重复 2是把全量变成增量
6. 新增一个卡的接口，具体需求需要问亮哥



20190701
------------------------------------------------------------------------------------------------------------------------
1. 第二季度的三份月报
2. 信贷部的7个抽数需求
3. 两个上线的开发单
4. wind数据解决方案
5. 互金项目接口开发
6. 风控接口文档整理
7. 上周的评审整理文档



下午安排：
1. 安排投产事宜  2个
2. 信贷抽数需求
3. 总结抽数单的抽数脚本

20190702
------------------------------------------------------------------------------------------------------------------------
安排：
1. 信贷抽数脚本整理
2. 互金项目的文档整理
3. 

完成
1. 信贷抽数需求完成
2. 环境检查完成


20190703
------------------------------------------------------------------------------------------------------------------------
1. 信贷抽数发现有两个字段没有，但是我不知道如何处理分析不出来

20190709
------------------------------------------------------------------------------------------------------------------------
1. 信贷抽数发现有两个字段没有，但是我不知道如何处理分析不出来    由于没找我了，就算了。不想自讨苦吃
2. 完成一个二类账户的数据抽取  激活网点没有


20190710
------------------------------------------------------------------------------------------------------------------------
1. 时间过的真快，三分之一的时间过去了。总结一下命名规范         评审已过


20190712
------------------------------------------------------------------------------------------------------------------------
1. 信贷又有抽数需求 问题是下周都没关系
2. 存款积数的抽取        已抽


私人任务：
黄金的数据采集

1. 使用聚合数据作为数据源
2. 使用shell脚本定期获取数据文件
3. 使用python将获取的数据文件改造可操作的数据文件
4. 使用kattle将改造过的数据文件入库
5. 改造流程，使之都能进行自动化操作



试试使用


全程使用shell把数据写进数据库中

1. 下载文件，每隔一个小时下载一个文件，0点不下，只创建文件夹
2. 每下载一个文件，都使用同一个名字，并且带有当天日期
3. 下载一个文件，就将文件改变格式，并且移动到另外一个文件夹中
4. 将改变过的文件重新读取，并生成del 格式，让另外一台服务器来取，并读入数据库


20190714
------------------------------------------------------------------------------------------------------------------------
1. 信贷数据
2. 征信数据


私人任务：
1. 测试时发现数据一直在20190612这天停了，很奇怪，是否需要再添加其他数据了呢？  已确认是源系统的问题

准备开发其他准确的数据

目前逻辑

云服A  下载文件到本地，处理成一个文件
云服B  从云服A获取文件 写入到数据库


积累脚本
1. 创建目录脚本
2. 下载数据脚本
3. 将json文件转化成txt文件的脚本
4. 从ftp服务器获取文件，并写入到数据库的脚本


重点：
1. 判断文件夹 文件是否存在的逻辑
2. 定时器的使用crontab
3. ftp 命令的使用
4. python 读取json文件的使用
5. db2 导入不同分隔符文件的语句使用


20190718
------------------------------------------------------------------------------------------------------------------------
数据已经得到了，而且昨天晚上用了一晚上的时间把历史数据全都放上去了

现在考虑的事情是：优化的事情，提升个人核心竞争力的事情上去


如何设计才能把所有机器的存储放在同一个机器上，即所有机器都能访问的。---有这个需求吗？




20190722
------------------------------------------------------------------------------------------------------------------------
考虑的问题:
1. 柜面现金券的要不要把历史数据保存起来   要
2. 工作问题
3. 如何实现 一个路径挂载在另一个系统上？

--操作
/home/hunter/ods
被挂载机（文件服务器）：47.107.234.54
挂载到机（实际上传文件机）：182.61.47.1

安装nfs服务
sudo apt-get install nfs-kernel-server
修改配置文件
sudo vi /etc/exports
*表示所有IP地址，rw:表示可读可写，sync:同步，no_root_squash:不降低root用户的权限
重启nfs服务
sudo /etc/init.d/nfs-kernel-server restart


vi /etc/exports，
加入：
/home/hunter/ods 182.61.47.1     (rw,sync,no_root_squash)  ;编辑需要共享的目录和允许访问的服务器及权限

service nfs start ;手动启动服务

然后到182.61.47.1
mkdir -p /home/hunter/ods
chmod 777 -R /home/hunter/ods
mount 47.107.234.54:/home/hunter/ods  /home/hunter/ods


mount -t nfs -o rw 47.107.234.54:/home/hunter/ods /home/hunter/ods

问题：挂载好长时间  也没反应
mount qiankunxu.com:/home/hunter/ods  /home/hunter/ods
    nfs 2049
    portmapper 111
1. 机器内修改
sudo ufw allow 111
sudo ufw allow 2049
2. 阿里云网页上添加规则
3. 重点是安装sudo apt-get install nfs-kernel-server

搞定

20190723
------------------------------------------------------------------------------------------------------------------------
1. 抽数单过期证件客户信息抽取
2. 信贷数据的脚本准备
3. CLM押品系统数据下发的开发单 文档整理 交付文档上传到svn
4. 抽数脚本的整理和脚本上传到svn


20190724
------------------------------------------------------------------------------------------------------------------------
date -d "20150416 -1 days" "+%Y%m%d"
20150415

date -d "20150416 -3 days" "+%Y%m%d"
20150413

date -d "20150416 3 days" "+%Y%m%d"
20150419


20190726
------------------------------------------------------------------------------------------------------------------------
想重新修改逻辑，将文件每隔一个小时就入库，增加实时性数据，而不是第二天才能看到
同时，我希望测试一下为什么总是连接不上数据库
.  /home/db2inst1/sqllib/db2profile

解决了这个问题，数据库可以连接上了，原因分析，没有环境，需要重新设置环境变量

我先做测试，之后再改回来，实时性 不满足也行，因为会造成重复数据

---做有挑战的事情，不要总是做重复的事情


二代支付的批量任务表  取好了，问题是字段多了几个，我们这里不影响，所以不管


20190729
------------------------------------------------------------------------------------------------------------------------
工作
1. 自助设备的统计报表开发
2. 信贷部的抽数需求


自助设备的开发涉及3个系统，统计数据需要进一步取生产确认



去重处理操作，每天处理去重
db2 "export to BANKGOLD_20190729.del of del
SELECT distinct VARIETY, MIDPRI, BUYPRI, SELLPRI, MAXPRI, MINPRI, TODAYOPEN, CLOSEYES, QUANTPRI, TIME, SYS_BIZ_DATE 
FROM ODS.BANKGOLD with ur"

db2 "export to shgold_20190729.del of del
select distinct VARIETY, LATESTPRI, OPENPRI, MAXPRI, MINPRI, LIMIT, YESPRI, TOTALVOL, TIME, SYS_BIZ_DATE
from ods.shgold with ur


价值从何而来呢

20190730
------------------------------------------------------------------------------------------------------------------------
1. 信贷数据     完成
2. 风险预警的文件中转
3. 超级柜台的报表优化
4. 自助设备的数量统计

20190801
------------------------------------------------------------------------------------------------------------------------
1. 超级柜台的报表优化
2. 自助设备的数量统计
3. 南海的wind数据接入
4. 风险预警系统的数据文件中转

20190806
------------------------------------------------------------------------------------------------------------------------
目前需要统一的数据是：
1. 手机号码实名认证接口
2. 全国快递物流数据接口
3. 全国车辆违章接口


目前可以开发的   识别
人脸识别技术
指纹识别技术



做一份可以出表格的数据，数据源取自聚合数据，展示使用excel

db2 "select VARIETY,OPENPRI,TOTALVOL,TIME from ods.shgold where VARIETY='AU99.99' with ur"

VARIETY     ,
LATESTPRI   ,
OPENPRI     ,
MAXPRI      ,
MINPRI      ,
LIMIT       ,
YESPRI      ,
TOTALVOL    ,
TIME        ,
SYS_BIZ_DATE,



db2 "export to shgold.txt of del select VARIETY     ,
LATESTPRI   ,
OPENPRI     ,
MAXPRI      ,
MINPRI      ,
LIMIT       ,
YESPRI      ,
TOTALVOL    ,
TIME        ,
SYS_BIZ_DATE from ods.shgold where VARIETY='AU99.99' with ur"

20190809
------------------------------------------------------------------------------------------------------------------------
个人的行为准则，

符合标准，制定可执行规范

完成开发任务的流程规范
1. 需求分析
2. 范围确认
3. 设计分析
4. 开发编码
5. 测试完成
6. 交付整理



20190813
------------------------------------------------------------------------------------------------------------------------
突然发现自己真的闲的慌  话说着就有点中年危机了，总觉得自己太闲了  心里不是滋味

20190814
------------------------------------------------------------------------------------------------------------------------
LiborPrices

PAYHSRCCBVLT




20190823
------------------------------------------------------------------------------------------------------------------------
打算设置一个密码本
使用密码本记录密码，防止密码找不到
固定工位名字作为密码
ODS02/Gdrcu14F016

20190826
------------------------------------------------------------------------------------------------------------------------
ubuntu sendmail配置发送邮件


ubuntu中sendmail函数可以很方便的发送邮件，ubuntu sendmail先要安装两个包。

必需安装的两个包：

代码 
sudo apt-get install sendmail  
sudo apt-get install sendmail-cf
 

下面几个包是可选的：

代码 
squirrelmail              //提供webmail          
spamassassin          //提供邮件过滤  
mailman                   //提供邮件列表支持  
dovecot                   // 提供IMAP和POP接收邮件服务器守护进程  
 

 

注意：

Ubuntu下使用最常用的mail功能，需要安装mailutils，
安装命令：sudo apt-get install mailutils  
使用带附件的功能，则还需要安装sharutils，
安装命令：sudo apt-get install sharutils；（yum install sharutils )
 

终端输入命令：ps aux |grep sendmail
输出如下：

root     20978  0.0  0.3   8300  1940 ?        Ss   06:34   0:00 sendmail: MTA: accepting connections          
root     21711  0.0  0.1   3008   776 pts/0    S+   06:51   0:00 grep sendmail

说明sendmail 已经安装成功并启动了

二、配置

sendmail 默认只会为本机用户发送邮件，只有把它扩展到整个Internet，才会成为真正的邮件服务器。

打开sendmail的配置宏文件：/etc/mail/sendmail.mc

 vi  /etc/mail/sendmail.mc

找到如下行：

代码 
DAEMON_OPTIONS(`Family=inet,  Name=MTA-v4, Port=smtp, Addr=127.0.0.1')dnl
 修改Addr=0.0.0.0  ，表明可以连接到任何服务器。

生成新的配置文件：

代码 
#cd /etc/mail  
#mv sendmail.cf sendmail.cf~      //做一个备份  
#m4 sendmail.mc > sendmail.cf   //>的左右有空格，提示错误没有安装sendmail-cf  
三、测试发送邮件

常用发送邮件方式如下：

    1.如何写一般的邮件： mail test@126.com  Cc 编辑抄送对象，Subject:邮件主题,输入回车，邮件正文后，按Ctrl-D结束

    2.快速发送方式： echo “邮件正文” | mail -s 邮件主题 test@126.com

    3.以文件内容作为邮件正文来发送： mail -s test test@126.com < test.txt

    4.发送带附件的邮件： uuencode 附件名称 附件显示名称 | mail -s 邮件主题 发送地址

       例如： uuencode test.txt test.txt | mail -s Test test@126.com

 

注意问题：

　　1)

　　如果你发现你的sendmail启动很慢的话,可以这样解决

　　# vi /etc/hosts

　　127.0.0.1 localhost.localdomain localhost

　　修改成

　　127.0.0.1 localhost.localdomain localhost 主机名称

　　查看本机的主机名称的命令是: hostname

　　只保留这一行就行,多余的行就可以删掉

　　2)　　

*** ERROR: FEATURE() should be before MAILER()
*** MAILER(`local') must appear after FEATURE(`always_add_domain')*** ERROR: FEATURE() should be before MAILER()
*** MAILER(`local') must appear after FEATURE(`allmasquerade')*** ERROR: FEATURE() should be before MAILER()

 

修改sendmail.mc最后几行
vi /etc/mail/sendmail.mc
将文档中的
MAILER_DEFINITIONS
MAILER(`local')dnl
MAILER(`smtp')dnl
放到文档最后（为什么这么做,没有找到原因，不过错误的提示,要求你这样做）

20190828
------------------------------------------------------------------------------------------------------------------------
密码本的数据库字典
1. usedfor
2. username
3. password
4. createdt
5. lastmdfy
6. remark

20190830
------------------------------------------------------------------------------------------------------------------------
应该准备自己的职业规划了，弄清楚接下来10年要做什么事情

182.61.47.1
96.0.27.13



20190903
------------------------------------------------------------------------------------------------------------------------
重视流程的总结
1. 输入数据
2. 输出数据

20190904
------------------------------------------------------------------------------------------------------------------------
不管什么事情都需要做一个准备，把事情准备好了，等到有机会的时候，就不用着急还有事情没完成了。


个人对银行的认识：
业务上，只要业务是


20190912
------------------------------------------------------------------------------------------------------------------------
DB2查看表空间和增加表空间容量

Db2 connect to xxx
Db2 “LIST TABLESPACES SHOW DETAIL”
Tablespace ID = 7
Name = TSASNAA
Type = Database managed space
Contents = All permanent data. Large table space.
State = 0x0000
Detailed explanation:[@more@]
Normal
Total pages = 14800
Useable pages = 14752
Used pages = 12864
Free pages = 1888
High water mark (pages) = 12864
Page size (bytes) = 8192
Extent size (pages) = 32
Prefetch size (pages) = 32
Number of containers = 1

增加表空间大小
DB2数据库使用时，如果表空间满了，该如何扩容呢？下文将教给您DB2数据库表空间扩容的方法，供您参考，希望对您有所帮助。
1）直接添加一个容器的例子：
db2 " ALTER TABLESPACE PAYROLL ADD (DEVICE '/dev/rhdisk9' 10000) "
加容器之后DB2会有一个自动balance的过程,可能会持续几个小时!!! 一定要注意该选项,修改前确认该选项是否能满足业务需求!
2）改变现有容器的大小(该方法不会触发balance,但如果表空间建立在裸设备上,则要扩冲裸设备空间):
db2 " ALTER TABLESPACE TS1 RESIZE (FILE '/conts/cont0' 2000, DEVICE '/dev/rcont1' 2000, FILE 'cont2' 2000) "
注意这种方式就是将原有的相应容器都改成大小是2000页
db2 "ALTER TABLESPACE TS1 RESIZE (ALL 2000)"
这种方式就是把表空间中所有的容器大小都改成2000页
db2 " ALTER TABLESPACE TS1 EXTEND (FILE '/conts/cont0' 1000, DEVICE '/dev/rcont1' 1000, FILE 'cont2' 1000) "
这种方式就是将相应的容器都扩大1000页，也就是增加1000页。
db2 " ALTER TABLESPACE DATA_TS EXTEND (ALL 1000)"
这种方式就是将所有的容器都增加1000页。


db2 "select substr(TABLESPACE_NAME,1,20) as TBSPC_NAME,bigint(TOTAL_PAGES * PAGE_SIZE)/1024/1024 as TOTAL(MB),
      used_pages*PAGE_SIZE/1024/1024 as USED(MB), free_pages*PAGE_SIZE/1024/1024 as FREE(MB)
    from table(snapshot_tbs_cfg('DB_NAME', -2)) as snapshot_tbs_cfg"



20190916
------------------------------------------------------------------------------------------------------------------------
1. ods.f_cif_psn_balance_sum_m 修改ds 修改数据表改为分区表  moia要添加一个增加分区的操作
2. 关于南海新增wind下发数据的需求  moia修改为增量  ds一个作业更新 cd确保能取到数据
3. piccs作业修改  主要是依赖关系和入库问题
4. 湛江 中间业务云的数据，要抽bcss的数据，设计TDA接口给报表使用
5. 中间业务云非税 的两个开发单



工作细分



20190916
------------------------------------------------------------------------------------------------------------------------
1. ods.f_cif_psn_balance_sum_m 修改ds 修改数据表改为分区表  moia要添加一个增加分区的操作
2. 关于南海新增wind下发数据的需求  moia修改为增量  ds一个作业更新 cd确保能取到数据
3. piccs作业修改  主要是依赖关系和入库问题
4. 湛江 中间业务云的数据，要抽bcss的数据，设计TDA接口给报表使用
5. 中间业务云非税 的两个开发单
6. 测试资管的CD服务是否可用
7. 过滤pos机


20190919
------------------------------------------------------------------------------------------------------------------------
1. ods.f_cif_psn_balance_sum_m 修改ds 修改数据表改为分区表  moia要添加一个增加分区的操作   （修改了ds作业，不报错了）
2. 关于南海新增wind下发数据的需求  moia修改为增量  ds一个作业更新 cd确保能取到数据
3. piccs作业修改  主要是依赖关系和入库问题
4. 湛江 中间业务云的数据，要抽bcss的数据，设计TDA接口给报表使用
5. 中间业务云非税 的两个开发单
6. 测试资管的CD服务是否可用
7. 过滤pos机



20190923
------------------------------------------------------------------------------------------------------------------------
1. ods.f_cif_psn_balance_sum_m 修改ds 修改数据表改为分区表  moia要添加一个增加分区的操作   （修改了ds作业，不报错了）
2. 关于南海新增wind下发数据的需求  moia修改为增量  ds一个作业更新 cd确保能取到数据
3. piccs作业修改  主要是依赖关系和入库问题
4. 湛江 中间业务云的数据，要抽bcss的数据，设计TDA接口给报表使用
5. 中间业务云非税 的两个开发单
6. 测试资管的CD服务是否可用      （已投产）
7. 过滤pos机


20190926
------------------------------------------------------------------------------------------------------------------------
1. ods.f_cif_psn_balance_sum_m  （已解决 配置一个moia作业sql直接插入到数据库）
2. 关于南海新增wind下发数据的需求   （已投产）  调整作业监控，循环取文件
3. piccs作业修改  主要入库问题 需要再次确认以下主键
4. 湛江 中间业务云的数据，要抽bcss的数据，设计TDA接口给报表使用 （等待测试）


20190926
------------------------------------------------------------------------------------------------------------------------
1. piccs作业修改  主要入库问题 需要再次确认以下主键


20191008
------------------------------------------------------------------------------------------------------------------------
1. 抽取移动支付交易数据

工作时间
20180723-20191008

20160112-20180720

数据库sql
基本的增删该查
稍微复杂点的关联查询以及子查询排序聚合函数的使用
安装过oracle数据库 mysql db2 等数据库

etl工具
熟练使用informatica datastage 等工具

编写过linux脚本，熟悉linux环境

很快能上手操作大数据工具，以及很强的学习能力，自学能力

业务上有过保险行业经验和银行开发经验

项目管理上已经获得过PMP证书

计算机专业毕业，有过多种编程语言经验，如后台编程语言java 前端编程语言JavaScript 脚本语言python

项目上参与过几个大型项目的开发，如友邦的泰国ODS项目 华为的管理这天兔项目 银信的ODS平台项目，主要作为ODS开发



--沟通技巧
1. 换位思考   站在对方的角度思考问题，考虑没有理解的问题以及原因，达到有效沟通
2. 抓住重点   把事情的根本原因分析清楚，在沟通过程中始终把握住重点，不说多余的话，提高沟通效率
3. 墨菲定律   把思考的到的可能都讲出来，一旦感觉可能发生的问题，一定就会发生，所以提前处理能提高做事效率
4. 控制情绪   交流过程中切记冲动，一旦情绪不满，则事情没法控制，得到的反馈也不会及时
5. 及时反馈   及时反馈的价值在于给领导充分的时间反应，尊重领导，能维持内部和谐
6. 多人沟通   多人沟通的时候，是一种场景，需要抓住某个人，有了责任意识，才会有反馈，否则，极易产生不清楚该不该说话的情况


跨部门沟通有效三点
--->信息共享
--->互惠信赖
--->充分授权


20191009
------------------------------------------------------------------------------------------------------------------------
评审一次性通过的策略
1. 分析过程
--确定范围 分析影响 
--->范围包括 moia作业 ds作业 shell脚本 sql脚本 新平台数据字典 设计文档 上线步骤等 
--->影响包括 上游系统 下游系统 ODS系统涉及其他作业

2. 设计过程
针对范围设计实现逻辑
-->确定时间 规则
-->确定最后作业规范
-->确定交付文档


3. 实施过程
4. 风险把控

20191011
------------------------------------------------------------------------------------------------------------------------
任务：
1.通知刘梦洁 piccs上线时间
2.王喆 互金项目接口确认上线路径
3.关于新增收单业务商户终端监控风控规则的需求开发单 计划2019年10月25日大版本上线

20191014
------------------------------------------------------------------------------------------------------------------------
任务：
1.通知刘梦洁 piccs上线时间
2.关于新增收单业务商户终端监控风控规则的需求开发单 计划2019年10月25日大版本上线


20191016
------------------------------------------------------------------------------------------------------------------------
任务：
1.piccs月底上线
2.关于新增收单业务商户终端监控风控规则的需求开发单 计划2019年10月25日大版本上线  20191017 退出大版本，30号投


简历突出：
ETL 工程师必备技能

1. Linux 使用
2. ETL 工具使用
3. 数据库sql语法等高级用法
4. 常用办公软件非常熟练


20191017
------------------------------------------------------------------------------------------------------------------------
针对一大堆工具的使用，编写描述，总结经验

由于网速限制，我是不可能把大文件放在云上的。所以我的服务器主要作用
1. 开发编码测试环境
2. 作为一台ETL跑批工具，从网站收集数据
3. 作为一个应用服务器，显示我的博客网站



20191018
------------------------------------------------------------------------------------------------------------------------
DS作业数据字段换行，在TF控件中对字段进行处理
Convert(char(13),'',Convert(char(10),'',NulltoEmpty(字段)))


20191021
------------------------------------------------------------------------------------------------------------------------
新平台上线，注意事项需要分析一下，保证一次性通过

https://help.aliyun.com/document_detail/117434.html?spm=a2c4g.11186623.2.17.4fdee499ZOke7m
ODS层设计规范




20191028
------------------------------------------------------------------------------------------------------------------------
1. 今天带了无线键盘和电脑支架到公司了

20191029
------------------------------------------------------------------------------------------------------------------------
1. 开发单--返回茂名历史数据，我也不知道数据在哪里
2. 开发单--给陆丰非税再接一张表，准备使用新平台接
3. 开发单--梅州非税报表数据有内部账的，需要改造 优化，我也不知道是哪些数据



20191104
------------------------------------------------------------------------------------------------------------------------
1. 非税报表优化统计口径
2. LINK关系表，由核心配合取数据             20191111 已经结单
3. 非税业务缴款的接入
4. 社保增加批量扣款的报表需求

20191111
------------------------------------------------------------------------------------------------------------------------
9月份    22天 176
10月份   19天 152
328


规范类问题：
1. 新接平台下发地市是发的什么数据字典文件，新格式的？
2. 派单的时候，是否要有我们判断是否给基础数据平台子单？
3. 新接的表是否依旧要使用moia的依赖条件给报表使用？



如何才算是熟悉hadoop？回答一下问题：
1. hadoop是什么？为什么要用hadoop？要在哪些场合使用hadoop？

2. hadoop的历史与现在的版本？

3. 环境搭建以及常用配置，是否能写出来？

4. hadoop生态有哪些？hadoop使用的名词解释？大数据平台的相关工具作用？

5. hadoop独自开发MR程序，是否能解决实际问题？


监控作业常见问题及处理
1. 数据库入库报错
2. 文件找不到报错
3. 源系统修改字段名或者字段类型报错
4. 下发作业空间不足报错



PDF便携式文档格式
衍生自PostScript 生成和输出图形
字符嵌入系统
结构化的存储系统

使用了工业标准的压缩算法


开发考虑点：
1. 环境因素
--源系统是否可连接
--用户名和密码是否有效
--数据表是否可访问

2. 业务因素
--业务产生的数据量大小
--数据是否会重复，是否有设置主键
--是否可有日期字段增量抽取
--是否有字段判断数据可拆分
--是否有大字段不需要取
--可分析出的字段是否有枚举值
--是否有必要的备注

3. 技术因素
--DS无法直抽mysql数据库
--数据类型之间的转换
--字段长度的设计是否满足需要
--审计字段6个必须放在最后
--常见的报错与解决方案注意
--检查作业的依赖是否正确
--分析开发作业带来的影响
--业务处理复杂逻辑是否测试通过
--上线步骤是否考虑完全

4. 流程因素
--考虑到任务完成需要预估时间
--业务数据来源系统如何找



基础算法：
算法一：快速排序算法
算法二：堆排序算法
算法三：归并排序
算法四：二分查找算法
算法五：BFPRT 线性查找算法
算法六：DFS 深度优先搜索
算法七：BFS 广度优先搜索
算法八：Dijkstra算法
算法九：动态规划算法
算法十：朴素贝叶斯分类算法


对于java编程，有些东西是要死记硬背的
1. 数据类型和精度
2. 字符串的常用方法必须记住
3. 常用库的常用方法必须记住
4. 常用算法需要能说明原理

20191112
------------------------------------------------------------------------------------------------------------------------

注销前备份文件步骤：
登录服务器qiankunxu.com 用户hunter
1. gold.log 将百度服务器的数据取出来备份
2. 将以下的脚本文件备份，并留下设置定时任务的格式

0 0 * * * sh /home/hunter/data/day_mkdir.sh
0 1-23/1 * * * sh /home/hunter/data/down_data.sh
0 8 * * * sh /home/hunter/data/weather_data.sh
5 1-23/1 * * * sh /home/hunter/data/down_shgold.sh

3. json2del.py 将对应解析json文件的python脚本备份

4. 备份网站步骤
-->备份文件路径/home/hunter/wwwroot 这个是部署文件
-->备份nginx设置文件/usr/local/nginx/conf/nginx.conf
-->记录工具对应的版本号

备份百度服务器的文件
登陆服务器182.61.47.1 用户huang
1. 将以下的脚本文件备份，并留下设置定时任务的格式
30 23 * * * sh /home/huang/ods/ftp_gold_data.sh
35 23 * * * sh /home/huang/ods/testdb.sh


主要备份目标：
1. 网站
2. 数据抓取程序以及相应数据
3. 过程中积累的文档


ETL工程师简历-黄进.doc
ETL工程师简历-黄进.pdf
yx当前任务.txt
个人总结.docx

20191113
------------------------------------------------------------------------------------------------------------------------
今天突然就想回家了，甚至不想工作了。
2020年春节 是1月25日 现在距离春节还有73天



大额手续费
小额手续费
金融服务平台


概念：农信银
全称：农信银支付清算系统
属性：支付清算平台


20191114
------------------------------------------------------------------------------------------------------------------------
报表的非税优化评审通过
报表的非税优化评审单子交付了

2020年春节 是1月25日 现在距离春节还有72天
探清水河   这首歌竟然不知不觉听了一整天



20191115
------------------------------------------------------------------------------------------------------------------------
因为昨天的歌曲，想起抖音，然后就想着自己去拍摄。作为爱好，应该是不错的
大版本要投产湛江日报社的那个报表单 先投产了吧
然后就不知道要干嘛了

常规脚本格式和默认脚本总结


1. 功能描述和作者信息以及编写日期等统一规范
2. 打印日志和传入参数检查等通用规范
3. 常用的脚本包括 文件是否压缩，所以该脚本有两个
-->创建ODS平台日期目录，包括目前所有的接入的源系统
-->从源系统取文件的通用脚本，循环检测文件是否存在
-->给目标系统推送文件的通用脚本
-->复制文件通用脚本，从这个路径复制到另外一个路径，并且生成就绪文件
-->检查源系统数据是否就绪的通用脚本  包括检测就绪文件和查询数据库的通用脚本
-->根据前置、柜面、中间业务云等需要汇集数据文件的通用脚本
-->根据前置、柜面、中间业务云等需要拆分数据文件的通用脚本
-->把所有的sta文件和sdm文件压缩的通用脚本
-->把所有的sta文件和sdm文件前20天的数据删除的通用脚本
-->数据库的表分区自动增加的通用脚本
-->将不同格式的文件统一转化成gbk格式的unix系统的数据文件 通用脚本


评审注意事项
1. 讲接入的数据业务模型
2. 讲字段长度大于255的字段设置原因
3. 讲数据接入来源和流向目标系统

评审讲三点：
1. 这个单做了什么，即上次我做了哪些内容
2. 这个单做了之后遇到了哪些问题，有没有解决方案，方案是否可行
3. 通过评审，这个单接下来将要做什么，下次是否需要评审


ODS必须关注
1. 是否有账号或者卡号
2. 是否有机构号或者法人号
3. 是否有大于255的长度的字段（讲清楚为何如此设置，即设置的必要）
4. 是否有对应的客户号，证件号
5. 是否有流水号或者其他唯一性标识



为了解决因沟通问题而造成很多理解上的误差，ODS采用检查表的方式作为规范，与源系统进行沟通
设计表格
1. 给提供数据的源系统需要填写的表格
源系统缩写和全拼                                  需要原因-->根据系统分类，规范作业，并且可以和源系统校对
数据具体有哪些表，提供表名和数据字典，数据库连接信息等    -->开发需要
数据就绪条件                                              -->调度需要，上线之后是运维维护，保证数据完整性
区分该条记录属于哪个地市的条件                            -->ODS开发规范，方便后续数据使用
数据服务于哪些业务                                        -->ODS提供抽数需求，需要了解数据业务，否则数据提供的不准确，
--降低数据抽错风险，以及节省抽数单需求分析等调研过程
源系统负责人                                              -->提供给运维作为后续数据维护

每项都填写完善，即可通过


2019年11月15日23:21:55
重新打开了之前开发的网站，发现自己写的东西，已经不敢保证自己很熟悉了，也不记得当时的想法了，
看着那些没有注释的代码，以及不熟悉的文件夹设置，我都不知道是什么意思了。
对于这种情况，我只能说，要么现在捡起来，要么，把文档做好，把注释写好。
同时，我发现我又想开始了解前端的知识了。要不要回顾一下呢？或者只把最重要的东西捡起来就好呢？
首先时间不允许，其次能力不允许，然后心很难沉静下来，就像当时写代码的时候，不能静下来是一样的
当时就不能静下来，现在更是不能，特别容易分心。

想法有三
1. 重新记忆那些正则表达式
2. 重新训练自己对于常用算法的使用
3. 工作时，很多时间是对需求来安排工作，自己的任务也可以通过这种形式展现出来，毕竟把问题描述出来，就已经解决了一半了

调度信息：
1. 全局规则
2. 执行频度  --启动窗口
3. 作业参数
4. 前置依赖

20191118
------------------------------------------------------------------------------------------------------------------------
保证金业务      对接第三方系统迁移到中间业务云
司法查控系统    实现账号冻结和解冻操作

经典模块化前端框架layui

通过看这个前端框架首页，想起了自己的网站，所以是不是要把这个网站做成自己喜欢的模样呢？

python(Django之html模板继承)

在编程的过程中，我们经常会重复性的写了很多的代码，比如一个页面的框架部分，这样我有多少个页面就得写上多少次，这样既不好维护，
也不够高效，所以我们引出了html的模板继承部分。

Django模版引擎中最强大也是最复杂的部分就是模版继承了
这个模板继承给理解上造成很大的困扰

你可以在我们的 官网首页 下载到 layui(类UI) 的最新版，它经过了自动化构建，更适合用于生产环境。目录结构如下：
  ├─css //css目录
  │  │─modules //模块css目录（一般如果模块相对较大，我们会单独提取，比如下面三个：）
  │  │  ├─laydate
  │  │  ├─layer
  │  │  └─layim
  │  └─layui.css //核心样式文件
  ├─font  //字体图标目录
  ├─images //图片资源目录（目前只有layim和编辑器用到的GIF表情）
  │─lay //模块核心目录
  │  └─modules //各模块组件
  │─layui.js //基础核心库
  └─layui.all.js //包含layui.js和所有模块的合并文件


https://github.com/sentsin/layui/    可获得源代码，进行二次开发
https://www.layui.com/doc/    layui开发使用文档

20191119
------------------------------------------------------------------------------------------------------------------------
消费贷的优化投产

后台管理系统开发
功能描述：
1. 用户管理
2. 权限管理
3. 注册登录

页面设计


20191119
------------------------------------------------------------------------------------------------------------------------
消费贷的优化投产   下周再投
中间业务云非税的大字段拆分脚本注意路径是新平台路径


后台管理系统开发
功能描述：
1. 用户管理
2. 权限管理
3. 注册登录

页面设计
1. 注册登录页面
2. 后台管理主页面
3. 前端主页面

分层设计--前端设计
分层设计--后台设计
分层设计--数据存储设计






网站利用数据库的主从热备功能，实现数据库读写分离，从而改善数据库负载压力
采用 CDN 和反向代理加快系统的静态资源访问速度
数据库采用分布式数据库，文件系统采用分布式文件系统
系统上按照业务进行拆分改造，应用服务器按照业务区分进行分别部署
公共业务提取出来，独立部署。由这些可复用的业务连接数据库，通过分布式服务提供共用业务服务

自动化
发布过程自动化
-->自动化代码管理
-->自动化测试
-->自动化安全监测
-->自动化部署

运维自动化
-->自动化监控
-->自动化报警
-->自动化失效转移
-->自动化失效恢复
-->自动化降级
-->自动化分配资源

安全
密码  和  手机校验码  进行身份认证
登录、交易等重要操作需要对网络通信进行  加密 ，存储的敏感数据如用户信息等也进行加密处理
防止机器人程序攻击网站，使用  验证码  进行识别
对常见用于  攻击  网站的 XSS 攻击、SQL 注入、进行编码转换等相应处理
对垃圾信息、敏感信息进行  过滤
对交易转账等重要操作根据交易模式和交易信息进行  风险控制

Power           Exact Value         Approx Value        Bytes---------------------------------------------------------------
7                             128
8                             256
10                           1024   1 thousand           1 KB
16                         65,536                       64 KB
20                      1,048,576   1 million            1 MB
30                  1,073,741,824   1 billion            1 GB
32                  4,294,967,296                        4 GB
40              1,099,511,627,776   1 trillion           1 TB

十四类工具  DevOps
编码版本控制：维护和控制源代码库中的变更
协作开发
构建：版本控制、代码合并、构建状态
持续集成
测试：自动化测试及测试报告
打包：二进制仓库、Docker镜像仓库
部署工具
容器：容器是轻量级的虚拟化组件，它以隔离的方式运行应用负载。它们运行自己的进程、文件系统和网络栈，这些资源都是由运行在硬件上的操作系统所虚拟化出来的
发布：变更管理、自动发布
编排：当考虑微服务、面向服务的架构、融合式基础设施、虚拟化和资源准备时，计算系统之间的协作和集成就称为编排。
      通过利用已定义的自动化工作流，编排保证了业务需求是和你的基础设施资源相匹配的
配置管理：基础设施配置和管理，维护硬件和软件最新的、细节的记录-包括版本、需求、网络地址、设计和运维信息
监视：性能监视、用户行为反馈
警告&分析工具
维护工具

DevOps核心思想就是：“快速交付价值，灵活响应变化”

工具名称	工具用途
Docker	    虚拟化容器技术，快速构建独立隔离服务
Jenkins	    自动化平台，可以配置自动化测试与持续部署
New Relic	应用性能监测
Ansible	    自动化(配置管理工具)平台，持续集成与编排
Kubernetes	开源的docker编排工具，可以发布与回滚
JMeter	    基于Java的压力测试工具。用于对软件做压力测试

20191120
------------------------------------------------------------------------------------------------------------------------
错误处理

db2 "import from /home/huang/ods/bankgold_20191118.json.txt of del modified by codepage=1208 COLDEL| compound=100 commitcount 10000 
MESSAGES /home/huang/log/bankgold_20191118.log insert into ods.bankgold"
db2 "import from /home/huang/ods/shgold_20191118.json.txt of del modified by codepage=1208 COLDEL| compound=100 commitcount 10000 
MESSAGES /home/huang/log/shgold_20191118.log insert into ods.shgold"

20191123
------------------------------------------------------------------------------------------------------------------------
对于站长来说，学会判断自身网站的价值性如何则是非常重要的

作为站长一定要搞清楚自己每天工作重点，做事情要提纲掣领，注重内容和外链建设，同时进行日常数据备份和安全管理，并且将这些工作列出计划表，
进而花费很少的时间完成，让运营网站变成一件很轻松的事情，这样就能够实现长期的坚持 


20191125
------------------------------------------------------------------------------------------------------------------------
监控数据：
1. 数据是否缺失，判断依据：记录都会带有日期字段，如果哪天的数据不存在，极有可能是数据缺失了
2. 数据数量监控，判断依据：增量数据一周的数据量和上周数据量会保持平衡，相差数据量可以控制在一个范围内
3. 数据枚举类型分布监控，根据业务分析得出数据的可能性，然后生成分布图

自动化运维工作
1. 日常检查工作  --监控
2. 配置变更工作  --实施
3. 软件安装工作  --开发
4. 自动测试开发

DevOps  实现开发运维一体化

跑批程序处理状态
000    执行成功
001    执行失败
002    启动检查
003    非执行日
004    正在处理
005    依赖不满足


数据库查询时间统计
1. 数据量200万 db2数据库第一次count(1) 需要11s  第二次200ms



然后到182.61.47.1
mkdir -p /home/hunter/ods
chmod 777 -R /home/hunter/ods

切换到root用户，执行命令即可(20191125)
mount 47.107.234.54:/home/hunter/ods  /home/hunter/ods

找出对应的名词解释以及对应的工具和应用即可
性能优化
负载均衡
高可用
高并发
高吞吐量
多线程核心源码原理
网络编程  --不针对某一编程语言，但是一定要熟悉一项语言



数据库问题性能优化
读写分离
分库分表


20191126
------------------------------------------------------------------------------------------------------------------------
使用MQ消息中间件
解耦、异步、削峰
Kafka是一种高吞吐量的分布式发布订阅消息系统
它具有低耦合、可靠投递、广播、流量控制、最终一致性等一系列功能

不足：
引入了MQ中间将势必会增加系统的复杂性，需要考虑更多的问题：如何保证消息没有重复消费？如何处理消息丢失的情况？如何保证消息传递的顺序性。
引发业务上一致性的问题：这也是由于异步解耦的，原本A的响应需要BC的成功，用同步操作是没有问题的，但如果为了提高响应在解耦后B成功而C调用失败，
而A已经给了客户成功的响应，这里就会有结果处理不一致的问题。

如何保证消息不被重复消费  消息ID的唯一性
如何处理消息丢失的问题    消息发送时的事务+确认机制
如何保证消息的顺序性      拆分多个Queue，每个Queue一个Consumer  每个消息消费者对应一个消息队列，保证顺序

Nginx
负载均衡
反向代理

技术都是解决问题的。

解耦  提高可扩展性
异步  提高系统性能
削峰  提高系统稳定性

客户信息分析
了解风俗文化，增加对话好感度和信任感

云服务器使用价值
1. 搭建电子商务网站   
2. 企业数据共享平台
3. 搭建应用服务器

搭建官网，提供外部访问接口平台

乾坤序首页改造
之前有个网站，是java开发的网站，使用的前端框架是bootsrap 所以是用，还是不用呢？

技术选型

-->前言
   -->搭建背景
   -->目的
   -->预期读者
-->需求分析
   -->个人需求描述
   -->实现方案选择
-->开发设计
   -->现有技术介绍
   -->展示页面设计
   -->关键技术编码
   -->遗留问题与解决方案
-->上线部署
   -->域名购买
   -->云服务器购买
   -->域名绑定与备案
   -->备案问题和解决过程
-->维护
   -->代码更新重启
   -->服务器到期数据备份
   -->文档总结
-->附录
   -->网站证书
   -->域名证书
   -->常用命令
-->参考文献


20191129
------------------------------------------------------------------------------------------------------------------------
更新一版计划文档
1. 必须要做的事情，报工记录登记，开发单总结
2. 环境密码登记，为以后测试提供资料
3. 机构法人代码表数据，提供后续检索
4. 下发配置，拆分策略配置表整理
5. 记住需要完成的任务登记 在此文档中，yx当前任务.txt
6. 需要整理的文档现总结如下
-->个人总结文档
-->yx当前任务
-->svn  开发单设计文档
-->svn  开发单测试文档
-->svn  开发单数据字典更新
-->svn  抽数单存档更新
-->svn  开发单上线步骤文档
-->周报报工文档
-->开发单需求分析txt文档

20191201
------------------------------------------------------------------------------------------------------------------------
今天有时间，将所有的文档重新整理了一下，不要的就直接删除


20191202
------------------------------------------------------------------------------------------------------------------------
待办事项
1. 中间业务云上线汕尾的非税基础表，给基础数据平台胡俊杰通知一声


20191203
------------------------------------------------------------------------------------------------------------------------
自助设备现金券别统计全量追数注意点
1. 备份数据
2. 查看数据量大小，大于1千万的数据，使用load方式最快
3. 追数之前写好追数步骤，清楚目前到哪一步了，进度是多少
4. 追全量数据注意交接日的数据，需要修改回日常跑批


咫尺天涯闲心
Huangjin12

20191205
------------------------------------------------------------------------------------------------------------------------
待办事项
1. 中间业务云上线汕尾的非税基础表，给基础数据平台胡俊杰通知一声
2. 自助设备现金券别统计上线需要全量追数
3. 清远供电局抽数据
4. 新批量代收付抽数据
5. 中间业务云整理


源系统推送文件出错，导致监控脚本超时，检查脚本运行日志，定位问题出现在哪里


想法：
通过看网站https://www.dandyweng.com/ 发现一个问题，那就是社交网站太多，如果创作出来一个作品
不想花太多时间去发布，而是通过社交网站的接口，批量发布，这需要很多时间，但是确实可以做些事情

20191206
------------------------------------------------------------------------------------------------------------------------
待办事项
1. 中间业务云上线汕尾的非税基础表，给基础数据平台胡俊杰通知一声
2. 自助设备现金券别统计查清数据原因，是不是因为漏数
3. 清远供电局抽数据
4. 新批量代收付抽数据
5. 中间业务云整理




20191209
------------------------------------------------------------------------------------------------------------------------
江门的旧帐号需要添加 12 13 14 15 16


20191211
------------------------------------------------------------------------------------------------------------------------
db2 "import from /home/huang/ods/bankgold_20191118.json.txt of del modified by codepage=1208 COLDEL, compound=100 
commitcount 10000 MESSAGES /home/huang/log/bankgold_20191118.log insert into ods.bankgold"


注意
codepage=1208   使用UTF8编码
codepage=1386   使用GBK编码


20191215
------------------------------------------------------------------------------------------------------------------------
/home/hunter/ods/20191208

/home/huang/ods


cp /home/hunter/ods/20191208/bankgold_20191208.json.txt /home/huang/ods
cp /home/hunter/ods/20191208/shgold_20191208.json.txt /home/huang/ods

cp /home/hunter/ods/20191209/bankgold_20191209.json.txt /home/huang/ods
cp /home/hunter/ods/20191209/shgold_20191209.json.txt /home/huang/ods

cp /home/hunter/ods/20191210/bankgold_20191210.json.txt /home/huang/ods
cp /home/hunter/ods/20191210/shgold_20191210.json.txt /home/huang/ods

cp /home/hunter/ods/20191211/bankgold_20191211.json.txt /home/huang/ods
cp /home/hunter/ods/20191211/shgold_20191211.json.txt /home/huang/ods

cp /home/hunter/ods/20191212/bankgold_20191212.json.txt /home/huang/ods
cp /home/hunter/ods/20191212/shgold_20191212.json.txt /home/huang/ods

cp /home/hunter/ods/20191213/bankgold_20191213.json.txt /home/huang/ods
cp /home/hunter/ods/20191213/shgold_20191213.json.txt /home/huang/ods

cp /home/hunter/ods/20191214/bankgold_20191214.json.txt /home/huang/ods
cp /home/hunter/ods/20191214/shgold_20191214.json.txt /home/huang/ods

db2 "import from /home/huang/ods/bankgold_20191118.json.txt of del modified by codepage=1208 COLDEL| compound=100 commitcount 10000 
MESSAGES /home/huang/log/bankgold_20191118.log insert into ods.bankgold"
db2 "import from /home/huang/ods/shgold_20191118.json.txt of del modified by codepage=1208 COLDEL| compound=100 commitcount 10000 
MESSAGES /home/huang/log/shgold_20191118.log insert into ods.shgold"


项目简介：
MIC营销洞察中心（ Marketing insight Center）
MIC助力营销数字化运营和精细化管理，为营销策划、执行、评估、优化提供端到端的数据支撑解决方案，
成为营销战略规划的决策引擎，战术执行的业务驱动力

本项目收集，拉通华为内外消费者用户数据，进行消费者用户画像，对用户进行细分，识别用户的兴趣爱好等特征，
并在此基础上进行用户分析、建立用户相关的业务应用系统、以及支持用户经营、营销、服务、产品的业务改进。




20191220
------------------------------------------------------------------------------------------------------------------------
下周安排：
1. 周一投产  导出cmis生产作业修改
2. 周二整理文档



20191223
------------------------------------------------------------------------------------------------------------------------
查询表性能


select table
,round((blocks*8)/1024,2)                     "高水位空间  M"
,round((new_rows*avg_row_len/1024/1024),2)    "真实适用空间  M"
from all_tables
where table_name='T_PAY_NCS_BOOK_A'


